{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220685f8",
   "metadata": {},
   "source": [
    "#### 1. What is prior probability? Give an example.\n",
    "Ans:\n",
    "    Prior probability, also known as prior distribution, refers to the initial probability assigned to an event or hypothesis before considering any evidence or specific information. It represents the belief or expectation about the likelihood of an event occurring based on existing knowledge or assumptions.\n",
    "\n",
    "An example of prior probability can be illustrated in the context of a medical diagnosis. Let's consider a rare disease that affects 1 in 10,000 individuals in a population. Before conducting any tests or obtaining any specific information about an individual, the prior probability of that person having the disease would be 1 in 10,000. This probability is based on general knowledge about the prevalence of the disease in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc767e0",
   "metadata": {},
   "source": [
    "#### 2.What is posterior probability? Give an example.\n",
    "Ans:\n",
    "    \n",
    "Posterior probability, in the context of Bayesian inference, refers to the updated probability of an event or hypothesis after taking into account new evidence or information. It is calculated by combining the prior probability with the likelihood of the evidence given the hypothesis.\n",
    "\n",
    "To illustrate the concept of posterior probability, let's consider the example of a medical test for a particular disease. Suppose the prior probability of an individual having the disease is 1 in 100 (0.01), based on general knowledge about the disease prevalence in the population.\n",
    "\n",
    "As example- \n",
    "Now, let's assume that the individual undergoes a diagnostic test for the disease, and the test result comes back positive. The next step is to calculate the posterior probability, which takes into account both the prior probability and the likelihood of obtaining a positive test result given the presence of the disease (also known as the sensitivity of the test).\n",
    "\n",
    "Suppose the sensitivity of the test is 0.95, meaning that the test correctly identifies 95% of individuals who have the disease. Given this sensitivity, we can calculate the posterior probability using Bayes' theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b120f605",
   "metadata": {},
   "source": [
    "#### 3.What is likelihood probability? Give an example.\n",
    "Ans:\n",
    "    Likelihood probability, in the context of statistical inference, refers to the probability of observing a particular set of data or evidence given a specific hypothesis or model. It quantifies the compatibility between the observed data and the hypothesis, indicating how well the hypothesis explains the observed data.\n",
    "\n",
    "To illustrate the concept of likelihood probability, let's consider a simple example of flipping a coin. Suppose we have a fair coin, and we want to determine the likelihood of obtaining a sequence of coin flips: H (heads) and T (tails).\n",
    "\n",
    "As example- \n",
    "\n",
    "Assuming the coin is fair, the probability of getting a head (H) or a tail (T) on any single flip is 0.5. Let's say we observe the following sequence of coin flips: HHTTH.\n",
    "\n",
    "The likelihood probability is the probability of obtaining this specific sequence of coin flips given the hypothesis that the coin is fair. We can calculate the likelihood by multiplying the individual probabilities of each flip:\n",
    "\n",
    "Likelihood probability = P(HHTTH | fair coin) = P(H) * P(H) * P(T) * P(T) * P(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148fbbe",
   "metadata": {},
   "source": [
    "#### 4.What is Naïve Bayes classifier? Why is it named so?\n",
    "Ans:\n",
    "    Naïve Bayes classifier is a simple probabilistic machine learning algorithm used for classification tasks. It is based on applying Bayes' theorem with the assumption of independence between the features (hence the term \"naïve\"). Despite this simplifying assumption, Naïve Bayes classifiers have proven to be effective in various practical applications, especially in text classification and spam filtering.\n",
    "\n",
    "The name \"Naïve Bayes\" comes from the naive assumption of feature independence made by the algorithm. It assumes that the presence or absence of a particular feature in a class is independent of the presence or absence of other features. In other words, it assumes that the features are conditionally independent given the class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82274936",
   "metadata": {},
   "source": [
    "#### 5.What is optimal Bayes classifier?\n",
    "Ans:\n",
    "    The optimal Bayes classifier, also known as the Bayes optimal classifier, is a theoretical concept in machine learning and pattern recognition. It represents the ideal classification algorithm that achieves the lowest possible error rate given perfect knowledge of the underlying data distribution.Mathematically, the optimal Bayes classifier can be defined as:\n",
    "\n",
    "Classify an input sample x as the class c that maximizes P(c | x).\n",
    "\n",
    "P(c | x) = (P(x | c) * P(c)) / P(x)\n",
    "\n",
    "In this formula, P(c | x) represents the posterior probability of class c given the observed features x. P(x | c) is the likelihood probability of observing the features x given class c, P(c) is the prior probability of class c, and P(x) is the probability of observing the features x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901857ae",
   "metadata": {},
   "source": [
    "#### 6. Write any two features of Bayesian learning methods.\n",
    "Ans:\n",
    "    Here are two features of Bayesian learning methods:\n",
    "\n",
    "Updateable Probabilistic Models: Bayesian learning methods allow for iterative model updates as new data becomes available. Instead of treating the model as fixed, Bayesian methods provide a systematic way to update the model's parameters and beliefs based on observed data. This iterative update process, known as Bayesian updating, allows the model to adapt and refine its predictions over time. It enables continuous learning and accommodates dynamic environments where the underlying data distribution may change.\n",
    "\n",
    "Incorporation of Prior Knowledge: Bayesian learning methods provide a mechanism to incorporate prior knowledge or beliefs into the learning process. Prior knowledge can be in the form of prior distributions over model parameters or hypotheses. By combining prior knowledge with observed data, Bayesian methods generate posterior distributions that represent the updated beliefs after considering both the prior knowledge and the data. This ability to incorporate prior knowledge allows Bayesian models to leverage existing information, domain expertise, or constraints, making them particularly useful when limited data is available or when prior knowledge is informative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6733526b",
   "metadata": {},
   "source": [
    "#### 7. Define the concept of consistent learners.\n",
    "Ans:\n",
    "    In machine learning, a consistent learner is a type of learning algorithm that converges to the true underlying concept or model as the amount of training data increases. In other words, a consistent learner produces increasingly accurate predictions or estimates as more data becomes available.\n",
    "\n",
    "Formally, a learning algorithm is said to be consistent if, given an infinite amount of training data, it will converge to the true model or concept with probability one. This means that as the sample size approaches infinity, the predictions made by the learner will match the true values with a high degree of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3354f419",
   "metadata": {},
   "source": [
    "#### 8. Write any two strengths of Bayes classifier.\n",
    "Ans:\n",
    "    Here are two strengths of the Bayes classifier:\n",
    "\n",
    "Efficient and Fast: Bayes classifiers are computationally efficient and fast in both training and prediction phases. They have a simple mathematical formulation and require minimal computational resources compared to more complex models. The calculation of class probabilities involves straightforward calculations and can be performed efficiently, making the Bayes classifier well-suited for real-time or large-scale applications.\n",
    "\n",
    "Handling High-Dimensional Feature Spaces: Bayes classifiers perform well in high-dimensional feature spaces, where the number of features is large relative to the available training data. They are robust to the curse of dimensionality, as the independence assumption (naïve assumption) allows the classifier to handle high-dimensional data effectively. Despite the simplified assumption, Bayes classifiers can achieve competitive performance in text classification, document categorization, and other applications involving high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14fab9",
   "metadata": {},
   "source": [
    "#### 9. Write any two weaknesses of Bayes classifier.\n",
    "Ans:\n",
    "    Here are two weaknesses of the Bayes classifier:\n",
    "\n",
    "Naïve Independence Assumption: The Bayes classifier assumes that all features are conditionally independent given the class label. This is known as the naïve independence assumption. In reality, many real-world problems have features that are dependent on each other. Violation of this assumption can lead to suboptimal performance and reduced accuracy of the classifier. \n",
    "\n",
    "Sensitivity to Feature Distribution: Bayes classifiers assume that features follow specific probability distributions, such as Gaussian distributions for continuous features or multinomial distributions for discrete features. If the true feature distribution deviates significantly from these assumptions, the Bayes classifier may not perform well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516cba65",
   "metadata": {},
   "source": [
    "### 10.Explain how Naïve Bayes classifier is used for Text classification , spam filtering & Market sentiment analysis\n",
    "Ans: \n",
    "    The Naïve Bayes classifier is a popular and effective algorithm for text classification tasks. Text classification involves assigning predefined categories or labels to textual documents based on their content. Here's how the Naïve Bayes classifier is used for text classification:\n",
    "\n",
    "Data Preparation: The first step is to prepare the training data. The training dataset consists of a set of labeled documents, where each document belongs to a specific category or class. The text in the documents is preprocessed, which typically involves tokenization, removing stop words, and applying techniques like stemming or lemmatization to normalize the text.\n",
    "\n",
    "Feature Extraction: Next, features are extracted from the preprocessed text to represent each document numerically. The most common approach is the bag-of-words representation, where each document is represented as a vector of word frequencies. Each unique word in the entire corpus forms a feature, and the frequency of each word in each document becomes the feature value. Other approaches like TF-IDF (Term Frequency-Inverse Document Frequency) can also be used to weigh the importance of each word in the document and across the entire corpus.\n",
    "\n",
    "Model Training: The Naïve Bayes classifier is trained using the labeled training data and the extracted features. The classifier estimates the probabilities required for classification based on the frequency of feature occurrences in each class. Specifically, it calculates the prior probabilities of each class based on the training data and the conditional probabilities of each feature given the class. The naïve independence assumption is applied, assuming that the features (words) are conditionally independent given the class label. This assumption simplifies the calculations by treating each word in the document independently.\n",
    "\n",
    "Evaluation: The performance of the Naïve Bayes classifier is assessed using evaluation metrics such as accuracy, precision, recall, or F1 score. The classifier's predictions are compared to the true labels of a held-out test dataset to measure its effectiveness in classifying the documents.\n",
    "\n",
    "The Naïve Bayes classifier's ability to handle large feature spaces, its computational efficiency, and its effectiveness in handling text data make it well-suited for spam filtering. By learning the patterns and frequencies of words in spam and non-spam emails, the classifier can make predictions on incoming emails and filter out unwanted spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a556fba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
